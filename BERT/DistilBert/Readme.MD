## Bert
- Bert (BiDirectional Encoder Representation from Transformers)
- They are Bi-Directional and can train itself with both the direction in 1 shot
- Has a very good Understanding regarding the context and language
- They generally vectorize the tokens in 768 * 1 vector
- They use BertPieceTokenizer to do it.
## DistilBert
- DistilBert are the miniature version of the Language Model BERT
- They are fast and gives pretty good scores with sufficient data
- They vectorize using DistilBertTokenizer
